{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664bcc9c-1347-4e16-899e-efac3babe5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#big dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b04c5a64-329f-4bba-8b4d-feddee84cc9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/07 11:33:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/07 11:33:53 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "/users/lyang439/.local/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, sum as _sum, collect_list, size, explode\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "spark = (SparkSession.builder.appName(\"pageRank4_enwiki_task1\")\n",
    "         .config(\"spark.driver.memory\", \"30g\")  # Sets the Spark driver memory to 30GB\n",
    "         .config(\"spark.executor.memory\", \"30g\")  # Sets the Spark executor memory to 30GB\n",
    "         .config(\"spark.executor.cores\", \"5\")  # Sets the number of cores for each executor to 5\n",
    "         .config(\"spark.task.cpus\", \"1\")  # Sets the number of cpus per task to be 1\n",
    "         .config(\"spark.eventLog.enabled\", \"true\")\n",
    "         .config(\"spark.eventLog.dir\", \"/mnt/data/spark-event-logs\")\n",
    "         .config(\"spark.local.dir\", \"/mnt/data/temp\") \n",
    "         .config(\"spark.dynamicAllocation.enabled\", \"true\") \n",
    "         .config(\"spark.shuffle.service.enabled\", \"true\") \n",
    "         .config(\"spark.dynamicAllocation.minExecutors\", \"2\") \n",
    "         .config(\"spark.dynamicAllocation.maxExecutors\", \"10\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Load data\n",
    "schema = StructType([\n",
    "    StructField(\"page\", StringType(), True),\n",
    "    StructField(\"link\", StringType(), True)\n",
    "])\n",
    "df = spark.read.csv(\"hdfs://10.10.1.1:9000/data/enwiki-pages-articles\", sep=\"\\t\", schema=schema).repartition(9)\n",
    "\n",
    "# Initialize page ranks\n",
    "pages = df.select(\"page\").distinct()\n",
    "links = df.groupBy(\"page\").agg(collect_list(\"link\").alias(\"links\"))\n",
    "ranks = pages.select(\"page\", lit(1).alias(\"rank\"))\n",
    "\n",
    "# Calculate PageRank\n",
    "for iteration in range(1):\n",
    "    contributions = links.join(ranks, \"page\").select(\"links\", (col(\"rank\") / size(\"links\")).alias(\"contribution\"))\n",
    "    contributions = contributions.withColumn(\"link\", explode(\"links\")).select(\"link\", \"contribution\")\n",
    "    \n",
    "    ranks = contributions.groupBy(\"link\").agg(_sum(\"contribution\").alias(\"sum_contributions\"))\n",
    "    ranks = ranks.select(col(\"link\").alias(\"page\"), (lit(0.15) + lit(0.85) * col(\"sum_contributions\")).alias(\"rank\"))\n",
    "    \n",
    "# Sort the ranks in descending order\n",
    "ranks = ranks.orderBy(col(\"rank\").desc())\n",
    "\n",
    "# Save results to HDFS\n",
    "ranks.write.format(\"csv\").mode(\"overwrite\").save(\"hdfs://10.10.1.1:9000/data/pageRank_enwiki_task1_res\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e42111-e436-4d94-b815-4ab0da2b5a40",
   "metadata": {},
   "source": [
    "#Small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40d18c41-59d2-4d75-915f-5916e610a43e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/07 16:53:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/07 16:53:39 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, sum as _sum, collect_list, size, explode\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "spark = (SparkSession.builder.appName(\"pageRank4_test0\")\n",
    "         .config(\"spark.driver.memory\", \"30g\")  # Sets the Spark driver memory to 30GB\n",
    "         .config(\"spark.executor.memory\", \"30g\")  # Sets the Spark executor memory to 30GB\n",
    "         .config(\"spark.executor.cores\", \"5\")  # Sets the number of cores for each executor to 5\n",
    "         .config(\"spark.task.cpus\", \"1\")  # Sets the number of cpus per task to be 1\n",
    "         .config(\"spark.eventLog.enabled\", \"true\")\n",
    "         .config(\"spark.eventLog.dir\", \"/mnt/data/spark-event-logs\")\n",
    "         .config(\"spark.local.dir\", \"/mnt/data/temp\") \n",
    "         .master(\"spark://10.10.1.1:7077\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Load data\n",
    "schema = StructType([\n",
    "    StructField(\"page\", IntegerType(), True),\n",
    "    StructField(\"link\", IntegerType(), True)\n",
    "])\n",
    "df = spark.read.csv(\"hdfs://10.10.1.1:9000/data/web-BerkStan.txt\", sep=\"\\t\", schema=schema)\n",
    "\n",
    "# Initialize page ranks\n",
    "pages = df.select(\"page\").distinct()\n",
    "links = df.groupBy(\"page\").agg(collect_list(\"link\").alias(\"links\"))\n",
    "ranks = pages.select(\"page\", lit(1).alias(\"rank\"))\n",
    "\n",
    "# Calculate PageRank\n",
    "for iteration in range(1):\n",
    "    contributions = links.join(ranks, \"page\").select(\"links\", (col(\"rank\") / size(\"links\")).alias(\"contribution\"))\n",
    "    contributions = contributions.withColumn(\"link\", explode(\"links\")).select(\"link\", \"contribution\")\n",
    "    \n",
    "    ranks = contributions.groupBy(\"link\").agg(_sum(\"contribution\").alias(\"sum_contributions\"))\n",
    "    ranks = ranks.select(col(\"link\").alias(\"page\"), (lit(0.15) + lit(0.85) * col(\"sum_contributions\")).alias(\"rank\"))\n",
    "    \n",
    "# Sort the ranks in descending order\n",
    "ranks = ranks.orderBy(col(\"rank\").desc())\n",
    "\n",
    "# Save results to HDFS\n",
    "ranks.write.format(\"csv\").mode(\"overwrite\").save(\"hdfs://10.10.1.1:9000/data/pageRank_test\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "917eed45-64b4-477e-b3f7-abdf25ccc85a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5764/2218380179.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f35347-b4c4-4ef6-982c-0882d95d0dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.4\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82d96971-55fb-487a-b9dd-00f08cbdc9b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project:AWB,1681.4274749151355\n",
      "WP:AES,1309.1734686406078\n",
      "Wikipedia:Coordinates in infoboxes,253.87409067113552\n",
      "Template:Redirect category shell,240.01823267528454\n",
      "Communes of France,230.43522094701353\n",
      "Category:Living people,204.76905768489385\n",
      "Wikipedia:Deletion review,201.98733390499268\n",
      "WP:GenFixes,181.57685401633913\n",
      "WP:HC,180.0640786991354\n",
      "France,158.7486275648395\n",
      ":Category:Taxonbar templates without from parameter,147.38347669763232\n",
      "Special:DoubleRedirects,146.67920551397154\n",
      "Animal,135.21504330296023\n",
      "Template:Taxonbar,134.62737697958732\n",
      "Germany,118.5322082580864\n",
      "Departments of France,118.31974423628651\n",
      "United States,116.2689638921831\n",
      "WT:TREE#Taxonbar addition requirements,112.42920223809493\n",
      "Arthropod,107.32486637050413\n",
      "bugzilla:42616,103.83009384868444\n",
      "Insect,97.90833227808594\n",
      "genus,90.32301848674041\n",
      "Wikipedia:Articles for deletion/PAGENAME (2nd nomination),88.91718941308602\n",
      "India,84.78876580155233\n",
      "WP:F5,81.30098652173973\n",
      "User:KolbertBot,78.67591421158285\n",
      "User:Tom.Bot/Task3,73.21172121389726\n",
      "List of Tachinidae genera,72.87674822279789\n",
      "en:User:COIBot#Blacklist,67.87745550789653\n",
      "Template:Coord,67.76639635840789\n",
      "User:AnomieBOT/docs/TemplateSubster,67.42798213197767\n",
      "Iran,65.09165177440752\n",
      "Lepidoptera,62.2722104804754\n",
      "Countries of the world,62.2268206191666\n",
      "Help:Using talk pages,61.940434990368566\n",
      "village,60.91740473847219\n",
      "Template talk:Taxonbar#from1,60.6528160772557\n",
      "States and territories of India,57.96526256168011\n",
      "Wikipedia:Wikidata,56.00053753832746\n",
      "User:RonBot/Run,54.758773893770204\n",
      "User:Addbot,54.31681676737304\n",
      "Counties of Iran,53.941330278302004\n",
      "Central European Time,53.58639319424009\n",
      "Italy,52.4913425204287\n",
      "List of countries,50.454167882736556\n",
      "moth,50.180775489064004\n",
      "Indian Standard Time,49.96648479335929\n",
      "Central European Summer Time,49.732733358960424\n",
      "Provinces of Iran,49.321389420086106\n",
      "Bakhsh,49.18903672688154\n",
      "Iran Standard Time,48.899437764326095\n",
      "Iran Daylight Time,48.74076808277884\n",
      "Postal Index Number,48.335111017081324\n",
      "WP:CFD,47.6995683631526\n",
      "Rural Districts of Iran,47.49040080494019\n",
      "WP:TW,46.7186326485793\n",
      "List of districts of India,46.5872270225863\n",
      "Romanize,46.46321474485118\n",
      "World War II,45.27356471148959\n",
      "Poland,44.308856362754796\n",
      "Voivodeships of Poland,41.64209087750272\n",
      "Powiat,40.47912314739601\n",
      "Gmina,40.30334517614753\n",
      "United Kingdom,39.902174516796606\n",
      "England,38.9457897262406\n",
      "Spain,38.013810798671756\n",
      "en:User:COIBot#Monitor list,37.64739230855396\n",
      "List of sovereign states,36.91671806355062\n",
      "Australia,36.79758677607077\n",
      "Canada,36.4678648275913\n",
      "Netherlands,35.450903226212574\n",
      "New York City,34.71408669942372\n",
      "WP:CFDS,34.35253040227895\n",
      "Karnataka,34.1146499886608\n",
      "London,33.364414700932194\n",
      "Eastern European Time,33.13614418943384\n",
      "Japan,32.69187508788987\n",
      "The New York Times,32.62869643692387\n",
      "User:Gene93k,31.911110639996622\n",
      "User talk:Gene93k,31.88047698079772\n",
      "Eastern European Summer Time,31.694069714379463\n",
      "English language,30.47989910817545\n",
      "\"Central Province, Sri Lanka\",29.91731956526672\n",
      "Noctuidae,29.85619126420306\n",
      "Brazil,29.164682053420698\n",
      "census,28.942042590463473\n",
      "Sri Lanka,28.714993339526817\n",
      "Category:Articles created by Qbugbot,28.395952378971316\n",
      "Russia,28.318548744526574\n",
      "en:User:COIBot#Whitelist,27.552692173107058\n",
      "Geometridae,27.34471814429161\n",
      "en:User:COIBot#Monitorlist,27.336458507409535\n",
      "en:User:COIBot#Domainredlist,27.336458507409535\n",
      "WP:NFUR,27.18166912222284\n",
      "China,27.07218015889585\n",
      "Sweden,26.40173027097753\n",
      "Norway,26.235688904800305\n",
      "California,26.06339501390669\n",
      "family (biology),25.66861221612094\n",
      "Species,25.457953528660283\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /data/pageRank_enwiki_0_res/* | head -n 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
